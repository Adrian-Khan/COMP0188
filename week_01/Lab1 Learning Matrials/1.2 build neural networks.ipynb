{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "X1gNgbkdlxon"
   },
   "source": [
    "# Build the Neural Network\n",
    "> To build the neural network, we need the framework of [PyTorch](https://pytorch.org/). You can refer above website for installation and other informations. After installation PyTorch, you can call torch to build your own models. Now, let's start.\n",
    "\n",
    "In this session, we'll build a simple feedforward neural network from scratch using the **PyTorch** library. Our goal is to create a model capable of classifying images from the FashionMNIST dataset.\n",
    "\n",
    "PyTorch is a powerful deep learning framework (used in leading labs like OpenAI, META, NVIDIA, Microsoft, DeepSeek, etc) that provides the tools we need to define, train, and test neural networks.\n",
    "\n",
    "\n",
    "\n",
    "In the lecture a simple feedforward neural network includes the input layer, hidden layer and output layer. \n",
    "\n",
    "In Pytorch, the [torch.nn](https://pytorch.org/docs/stable/nn.html) can be used to constructed the neural network models. \n",
    "The [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) is the base class for all neural network modules, which contains layers, and a method ``forward(input)`` that returns the <b>output</b>.\n",
    " Let's have a look the details. \n",
    "\n",
    "\n",
    "In the next section, we'll build a neural network to classify images in the FashionMNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 10221,
     "status": "ok",
     "timestamp": 1677773883620,
     "user": {
      "displayName": "陈振",
      "userId": "06999655275882771177"
     },
     "user_tz": 0
    },
    "id": "A_1UNIuslxoo"
   },
   "outputs": [],
   "source": [
    "# --- Core PyTorch Imports ---\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F # Contains useful functions like activation functions\n",
    "\n",
    "# --- Data Handling Imports ---\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# --- Other Essential Imports ---\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mDmBMdezlxop"
   },
   "source": [
    "## Get Device for Training model\n",
    "\n",
    "PyTorch can leverage hardware accelerators like GPUs for faster training. We'll write our code to be device-agnostic, meaning it will run on a GPU if one is available (cuda) or fall back to the CPU if not.\n",
    "\n",
    "\n",
    "Let's check to see if\n",
    "[torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html) is available, else we\n",
    "continue to use the CPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1677773887944,
     "user": {
      "displayName": "陈振",
      "userId": "06999655275882771177"
     },
     "user_tz": 0
    },
    "id": "Htn4hdl9lxop",
    "outputId": "2e2ebc44-a234-4b54-9031-0eb430436340"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# choose the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YpxZUavAlxop"
   },
   "source": [
    "## 1- Define the Neural Network\n",
    "\n",
    "In PyTorch, we define our network as a Python class that inherits from `nn.Module`. `nn.Module` is the base class for all neural network modules.\n",
    "\n",
    "Our class will have two essential parts:\n",
    "1.  `__init__()`: This is where we define the layers of our network (e.g., linear layers, flatten layers).\n",
    "2.  `forward()`: This is where we specify how data flows through the layers we defined.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # Initialize the attributes of parent class\n",
    "\n",
    "        # This layer flattens the 28x28 images into a 784-dimensional vector\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # This is an ordered container of layers.\n",
    "        # Data will pass through them in the sequence they are defined.\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # First fully connected layer: 784 inputs, 512 outputs\n",
    "            nn.Linear(28*28, 512),\n",
    "            # ReLU activation to introduce non-linearity\n",
    "            nn.ReLU(),\n",
    "            # Second fully connected layer: 512 inputs, 512 outputs\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            # Output layer: 512 inputs, 10 outputs (one for each class)\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First, flatten the input image\n",
    "        x = self.flatten(x)\n",
    "        # Then, pass the flattened input through the sequential layers\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ls24cvOilxoq"
   },
   "source": [
    "Now, we create an instance of ``NeuralNetwork``, i.e. ``model``, and move it to the ``device``, then print its structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1677773893162,
     "user": {
      "displayName": "陈振",
      "userId": "06999655275882771177"
     },
     "user_tz": 0
    },
    "id": "K8TRUuNAlxor",
    "outputId": "27a056e2-5cb9-42e1-f5c8-c318916cf0dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of our network\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A Walk-Through of the Forward Pass (How Data Flows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic, we have defined our neural network. Let's trace what happens when we pass a single image to our model. It's always good practice to sanity check our models too. The output of the final linear layer is a tensor of raw prediction values called **logits**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([1, 10])\n",
      "Raw logits: tensor([[ 0.0314,  0.0141,  0.0244, -0.1263,  0.1768, -0.0404, -0.0989,  0.1189,\n",
      "         -0.0509,  0.0812]], grad_fn=<AddmmBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy input tensor representing one 28x28 image\n",
    "# The '1' is the batch size.\n",
    "X = torch.randn(1, 28, 28, device=device)\n",
    "\n",
    "# Pass the dummy input through the model to get the logits\n",
    "logits = model(X)\n",
    "print(f\"Logits shape: {logits.shape}\")  # Should be [1, 10] for batch size 1 and 10 classes\n",
    "print(f\"Raw logits: {logits}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logits are not probabilities**! To get probabilities, we apply the Softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities shape: torch.Size([1, 10])\n",
      "Predicted probabilities: tensor([[0.1014, 0.0997, 0.1007, 0.0866, 0.1173, 0.0944, 0.0890, 0.1107, 0.0934,\n",
      "         0.1066]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Predicted class: 4\n"
     ]
    }
   ],
   "source": [
    "# The softmax function converts logits into a probability distribution over the classes\n",
    "pred_probabilities = F.softmax(logits, dim=1)\n",
    "print(f\"Predicted probabilities shape: {pred_probabilities.shape}\")  # Should be [1, 10]\n",
    "print(f\"Predicted probabilities: {pred_probabilities}\\n\")\n",
    "\n",
    "# To get the predicted class, we take the index of the highest probability.\n",
    "y_pred = pred_probabilities.argmax(1)\n",
    "print(f\"Predicted class: {y_pred.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Training Components: Loss and Optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a network, we need two more things:\n",
    "- **Loss Function**: Measures how far the model's output (logits) is from the actual target (the correct class). For multi-class classification, the standard choice is nn.CrossEntropyLoss.\n",
    "\n",
    "- **Optimizer**: Implements an algorithm (like Stochastic Gradient Descent) to adjust the model's internal parameters (weights and biases) to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loss Function ---\n",
    "# CrossEntropyLoss is ideal for classification. It internally applies Softmax,\n",
    "# so we should feed it the raw logits directly from our model.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Optimizer ---\n",
    "# We'll use Stochastic Gradient Descent (SGD).\n",
    "# We pass model.parameters() to tell the optimizer which values it needs to update.\n",
    "# The 'lr' is the learning rate, a crucial hyperparameter.\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Full Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a neural network involves a loop where we repeatedly perform these five steps:\n",
    "\n",
    "1.  **Forward Pass:** Get the model's predictions (logits).\n",
    "2.  **Calculate Loss:** Compare the predictions to the true labels.\n",
    "3.  **Zero Gradients:** Clear old gradients from the previous step.\n",
    "4.  **Backward Pass (Backpropagation):** Calculate the gradient of the loss with respect to each model parameter.\n",
    "5.  **Update Weights:** The optimizer adjusts the parameters based on the calculated gradients.\n",
    "\n",
    "Let's simulate a single training step with a dummy input and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Loss: 2.4460151195526123\n",
      "Model weights have been updated after one step.\n"
     ]
    }
   ],
   "source": [
    "# Let's create a dummy target label (e.g., the true class is 3)\n",
    "target = torch.tensor([3], dtype = torch.long, device = device)\n",
    "\n",
    "# --- A single training step --- (We typically repeat this in a loop over many epochs)\n",
    "\n",
    "# 1. Forward Pass: Get the model's output for a dummy input\n",
    "logits = model(X)\n",
    "\n",
    "# 2. Compute Loss: Compare the model's output with the target\n",
    "loss = loss_fn(logits, target)\n",
    "print(f\"Calculated Loss: {loss.item()}\")\n",
    "\n",
    "# 3. Zero Gradients: Clear previous gradients before backpropagation\n",
    "# We need to reset the gradients before backpropagation, or they will accumulate.\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 4. Backward Pass: Compute gradients of the loss with respect to model parameters\n",
    "loss.backward()\n",
    "\n",
    "# 5. Update Weights\n",
    "# The optimizer adjusts the model's parameters using the gradients computed in the backward pass.\n",
    "optimizer.step() # weights = weights - learning_rate * gradients\n",
    "\n",
    "print(\"Model weights have been updated after one step.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've successfully defined a simple feedforward neural network, traced a forward pass, and outlined the components needed for training. Now let's inspect each part in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going into the details of each section above one by one."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bGrEGhEolxos"
   },
   "source": [
    "## Each Layer Analysis\n",
    "\n",
    "Now, we analyze each layer in the model. \n",
    "To illustrate it, we will take a sample minibatch of 3 with size 28x28 using [nn.Rand](https://pytorch.org/docs/stable/generated/torch.rand.html). Then we pass it through the network and do the further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1677773898988,
     "user": {
      "displayName": "陈振",
      "userId": "06999655275882771177"
     },
     "user_tz": 0
    },
    "id": "81WSWX8rlxos",
    "outputId": "0b786092-7ccd-4f5d-88ae-a9a214b39f24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand(3,28,28)\n",
    "print(input.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EAjZq5Knlxos"
   },
   "source": [
    "### nn.Flatten\n",
    "We initialize the [nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layer to convert each 28x28 input into a contiguous array of 784 values (\n",
    "the minibatch dimension (at dim=0) is maintained).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1677773903032,
     "user": {
      "displayName": "陈振",
      "userId": "06999655275882771177"
     },
     "user_tz": 0
    },
    "id": "WMPR7R1wlxos",
    "outputId": "4289d8d3-acc7-4947-f33d-a92e07bd1890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_input = flatten(input)\n",
    "print(flat_input.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KYOKLlklxos"
   },
   "source": [
    "### nn.Linear\n",
    "The [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "is a module that applies a linear transformation on the input using its stored weights and biases.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 214,
     "status": "ok",
     "timestamp": 1677773905453,
     "user": {
      "displayName": "陈振",
      "userId": "06999655275882771177"
     },
     "user_tz": 0
    },
    "id": "qHdpLSSplxot",
    "outputId": "1480d312-bc63-404a-dc96-6115eb75c2a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_input)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPxymVzxlxot"
   },
   "source": [
    "### nn.ReLU\n",
    "Non-linear activations are what create the complex mappings between the model's inputs and outputs.\n",
    "They are applied after linear transformations to introduce *nonlinearity*, helping neural networks\n",
    "learn a wide variety of phenomena.\n",
    "\n",
    "In this model, we use [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) between our\n",
    "linear layers, but there's other activations to introduce non-linearity in your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1677773907680,
     "user": {
      "displayName": "陈振",
      "userId": "06999655275882771177"
     },
     "user_tz": 0
    },
    "id": "1hHIrZkolxot",
    "outputId": "025b51fe-ba63-47aa-9b58-6583266084f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.1230, -0.0530, -0.0769,  0.0015,  0.2978,  0.2154,  0.3731, -0.0948,\n",
      "          0.0743, -0.0480, -0.0471, -0.2434, -0.3886,  0.3078,  0.9808,  0.2476,\n",
      "         -0.9264,  0.1047, -0.1490,  0.0292],\n",
      "        [ 0.1122,  0.0870, -0.0689,  0.0156,  0.2387,  0.1687,  0.4041, -0.2142,\n",
      "          0.2437, -0.1256,  0.1172, -0.2881, -0.2813,  0.2007,  1.0718,  0.6410,\n",
      "         -0.8452,  0.1147,  0.1220,  0.0767],\n",
      "        [ 0.0705,  0.0964,  0.2674, -0.1119, -0.0634, -0.2818,  0.1298, -0.0944,\n",
      "          0.4618,  0.1933,  0.1285, -0.2135, -0.3047, -0.0992,  1.0859,  0.4416,\n",
      "         -0.5967,  0.0672, -0.1959,  0.1097]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.0000, 0.0000, 0.0015, 0.2978, 0.2154, 0.3731, 0.0000, 0.0743,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.3078, 0.9808, 0.2476, 0.0000, 0.1047,\n",
      "         0.0000, 0.0292],\n",
      "        [0.1122, 0.0870, 0.0000, 0.0156, 0.2387, 0.1687, 0.4041, 0.0000, 0.2437,\n",
      "         0.0000, 0.1172, 0.0000, 0.0000, 0.2007, 1.0718, 0.6410, 0.0000, 0.1147,\n",
      "         0.1220, 0.0767],\n",
      "        [0.0705, 0.0964, 0.2674, 0.0000, 0.0000, 0.0000, 0.1298, 0.0000, 0.4618,\n",
      "         0.1933, 0.1285, 0.0000, 0.0000, 0.0000, 1.0859, 0.4416, 0.0000, 0.0672,\n",
      "         0.0000, 0.1097]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgzfQr1ilxot"
   },
   "source": [
    "### nn.Sequential\n",
    "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered\n",
    "container of modules. The data is passed through all the modules in the same order as defined. You can use\n",
    "sequential containers to put together a quick network like ``seq_modules``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 902,
     "status": "ok",
     "timestamp": 1677773913459,
     "user": {
      "displayName": "陈振",
      "userId": "06999655275882771177"
     },
     "user_tz": 0
    },
    "id": "V-niVeeilxot"
   },
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input = torch.rand(3,28,28)\n",
    "\n",
    "logits = seq_modules(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tn0oRqolxot"
   },
   "source": [
    "### nn.Softmax\n",
    "The last linear layer of the neural network returns `logits` - raw values in $[-\\infty, \\infty]$ - which are passed to the\n",
    "[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) module. The logits are scaled to values\n",
    "[0, 1] representing the model's predicted probabilities for each class. ``dim`` parameter indicates the dimension along\n",
    "which the values must sum to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1677773915723,
     "user": {
      "displayName": "陈振",
      "userId": "06999655275882771177"
     },
     "user_tz": 0
    },
    "id": "xewj11rGlxot"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0792, 0.0943, 0.0843, 0.1303, 0.0663, 0.1243, 0.1237, 0.0889, 0.0911,\n",
      "         0.1176],\n",
      "        [0.0838, 0.1091, 0.0902, 0.1291, 0.0605, 0.1157, 0.1373, 0.0907, 0.0770,\n",
      "         0.1065],\n",
      "        [0.0747, 0.1039, 0.0906, 0.1271, 0.0613, 0.1168, 0.1375, 0.0846, 0.0832,\n",
      "         0.1204]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "print(pred_probab)\n",
    "print(pred_probab.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters\n",
    "\n",
    "In the model, we define the ``forward function`` in ``NeuralNetwork``, the backward function can be automatically calculated by ``autograd`` in PyTorch. \n",
    "\n",
    "Many layers of a model are *parameterized*, i.e. have associated weights\n",
    "and biases that are optimized during training. Subclassing ``nn.Module`` automatically\n",
    "tracks all fields defined inside your model object, and makes all parameters\n",
    "accessible using your model's ``parameters()`` or ``named_parameters()`` methods.\n",
    "\n",
    "In this example, we iterate over each learnable parameter, and print its size and its values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "torch.Size([512, 784])\n",
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight\n",
      "  Size: torch.Size([512, 784])\n",
      "  First few values:\n",
      " tensor([[ 1.9848e-02,  2.6076e-03,  8.3389e-03,  ..., -1.6783e-03,\n",
      "          2.1267e-02,  1.3748e-03],\n",
      "        [ 1.0714e-05,  8.8581e-03, -3.1278e-03,  ..., -7.7031e-03,\n",
      "          3.2129e-02, -1.6902e-02]])\n",
      "\n",
      "Layer: linear_relu_stack.0.bias\n",
      "  Size: torch.Size([512])\n",
      "  First few values:\n",
      " tensor([0.0173, 0.0096])\n",
      "\n",
      "Layer: linear_relu_stack.2.weight\n",
      "  Size: torch.Size([512, 512])\n",
      "  First few values:\n",
      " tensor([[-0.0376, -0.0024, -0.0102,  ..., -0.0155, -0.0090,  0.0294],\n",
      "        [-0.0061,  0.0423,  0.0049,  ...,  0.0220,  0.0214,  0.0081]])\n",
      "\n",
      "Layer: linear_relu_stack.2.bias\n",
      "  Size: torch.Size([512])\n",
      "  First few values:\n",
      " tensor([0.0033, 0.0031])\n",
      "\n",
      "Layer: linear_relu_stack.4.weight\n",
      "  Size: torch.Size([10, 512])\n",
      "  First few values:\n",
      " tensor([[-0.0127, -0.0248,  0.0326,  ...,  0.0301, -0.0075,  0.0056],\n",
      "        [ 0.0060,  0.0028,  0.0275,  ...,  0.0313, -0.0281, -0.0314]])\n",
      "\n",
      "Layer: linear_relu_stack.4.bias\n",
      "  Size: torch.Size([10])\n",
      "  First few values:\n",
      " tensor([-0.0364, -0.0067])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the learnable parameters\n",
    "params = list(model.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())\n",
    "\n",
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "for name, param in model.named_parameters():\n",
    "    # A parameter is \"learnable\" if requires_grad is True\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name}\")\n",
    "        print(f\"  Size: {param.size()}\")\n",
    "        print(f\"  First few values:\\n {param.data[:2]}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the Weights of the networks\n",
    "After we obtain the parameters by feeding input into network, we can update the parameters based on the optimizer.\n",
    "\n",
    "In practice, the simplest update rule is the Stochastic Gradient Descent (SGD):\n",
    "\n",
    "    ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "By updating the parameters, we complete the whole training process and achieve the best parameters.\n",
    "\n",
    "This is done using [torch.optim](https://pytorch.org/docs/stable/optim.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gK7Uw7Dalxou"
   },
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3KPAfqzqlxou"
   },
   "source": [
    "## Further Reading\n",
    "You can refer the following website for further information.\n",
    "\n",
    "- [torch.nn API](https://pytorch.org/docs/stable/nn.html)\n",
    "- [tutorials](https://pytorch.org/tutorials/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (foundations_of_ai_comp0186)",
   "language": "python",
   "name": "foundations_of_ai_comp0186"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
